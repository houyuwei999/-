{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06c5f8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114    No no:)this is kallis home ground.amla home to...\n",
      "3589    I am in escape theatre now. . Going to watch K...\n",
      "3095    We walked from my moms. Right on stagwood pass...\n",
      "1012       I dunno they close oredi not... ÌÏ v ma fan...\n",
      "3320                               Yo im right by yo work\n",
      "4130    \\Its Ur luck to Love someone. Its Ur fortune t...\n",
      "1197     He also knows about lunch menu only da. . I know\n",
      "5426        Oh yeah! And my diet just flew out the window\n",
      "624     Nah it's straight, if you can just bring bud o...\n",
      "2260    SplashMobile: Choose from 1000s of gr8 tones e...\n",
      "Name: v2, dtype: object\n",
      "1114     ham\n",
      "3589     ham\n",
      "3095     ham\n",
      "1012     ham\n",
      "3320     ham\n",
      "4130     ham\n",
      "1197     ham\n",
      "5426     ham\n",
      "624      ham\n",
      "2260    spam\n",
      "Name: v1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# data wrangling\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read data\n",
    "data_dir = 'C:/Users/Dell/kaggle_dataset'\n",
    "df = pd.read_csv(data_dir+'/spam.csv', encoding='latin-1')\n",
    "\n",
    "# Train test split\n",
    "data_train, data_test,labels_train, labels_test = train_test_split(df.v2, df.v1, test_size=0.2, random_state=0)\n",
    "print(data_train[:10])\n",
    "print(labels_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1b586d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words:11706\n"
     ]
    }
   ],
   "source": [
    "# Find the unique vocabulary\n",
    "def GetVocabulary(data):\n",
    "    vocab_dict = {}\n",
    "    wid = 0\n",
    "    for document in data:\n",
    "        words = document.split()\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word not in vocab_dict:\n",
    "                vocab_dict[word] = wid\n",
    "                wid+=1\n",
    "    return vocab_dict\n",
    "\n",
    "# 用训练集建立词汇表\n",
    "vocab_dict = GetVocabulary(data_train)\n",
    "print('Number of unique words:' + str(len(vocab_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f66539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4457\n"
     ]
    }
   ],
   "source": [
    "# 词汇表向量统计\n",
    "def Document2Vector(vocab_dict, data):\n",
    "    word_vector = np.zeros(len(vocab_dict.keys()))\n",
    "    words = data.split()\n",
    "    out_of_voc = 0\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in vocab_dict:\n",
    "            word_vector[vocab_dict[word]] += 1\n",
    "        else:\n",
    "            out_of_voc += 1\n",
    "    return word_vector, out_of_voc\n",
    "\n",
    "train_matrix = []\n",
    "for document in data_train.values:\n",
    "    word_vector,_ = Document2Vector(vocab_dict, document)\n",
    "    train_matrix.append(word_vector)\n",
    "print(len(train_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d92c1df",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (<ipython-input-67-ff2c93b37aad>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-67-ff2c93b37aad>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs), spam_total_count, ham_total_count = NaiveBayes_train(train_matrix, labels_train.values)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "def NaiveBayes_train(train_matrix, labels_train):\n",
    "    num_docs = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    \n",
    "    spam_word_counter = np.ones(num_words)\n",
    "    ham_word_counter = np.ones(num_words)\n",
    "    \n",
    "    spam_total_count = 0\n",
    "    ham_total_count = 0\n",
    "    \n",
    "    spam_count = 0\n",
    "    ham_count = 0\n",
    "    \n",
    "    for i in range(num_docs):\n",
    "        if i%500 == 0:\n",
    "            print('Train on the doc id'+str(i))\n",
    "        if labels_train[i] == 'ham':\n",
    "            spam_word_counter += train_matrix[i]\n",
    "            spam_total_count += sum(train_matrix[i])\n",
    "            spam_count += 1\n",
    "        else:\n",
    "            ham_word_counter += train_matrix[i]\n",
    "            ham_total_count += sum(train_matrix[i])\n",
    "            ham_count += 1\n",
    "    \n",
    "    p_spam_vector = np.log(spam_word_counter/(spam_total_count + num_words))\n",
    "    p_ham_vector = np.log(ham_word_counter/(ham_total_count + num_words))\n",
    "    \n",
    "    return p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs), spam_total_count, ham_total_count\n",
    "# Train data as a model\n",
    "p_spam_vector, np.log(spam_count/num_docs), p_ham_vector, np.log(ham_count/num_docs), spam_total_count, ham_total_count = NaiveBayes_train(train_matrix, labels_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0911be2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test on the doc id:0\n",
      "Test on the doc id:200\n",
      "Test on the doc id:400\n",
      "Test on the doc id:600\n",
      "Test on the doc id:800\n",
      "Test on the doc id:1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham, spam_smoothing, ham_smoothing):\n",
    "    spam = sum(test_word_vector * p_spam_vector) + p_spam + spam_smoothing\n",
    "    ham = sum(test_word_vector * p_ham_vector) + p_ham + ham_smoothing\n",
    "    if spam > ham:\n",
    "        return 'spam'\n",
    "    else:\n",
    "        return 'ham'\n",
    "\n",
    "num_words = len(vocab_dict.keys())\n",
    "prediction = []\n",
    "i = 0\n",
    "\n",
    "for document in data_test.values:\n",
    "    if i%200 == 0:\n",
    "        print('Test on the doc id:' + str(i))\n",
    "    i += 1\n",
    "    test_word_vector, out_of_vector = Document2Vector(vocab_dict, document)\n",
    "    \n",
    "    if out_of_vector != 0:\n",
    "        spam_smoothing = np.log(out_of_vector/(spam_total_count+num_words))\n",
    "        ham_smoothing = np.log(out_of_vector/(ham_total_count+num_words))\n",
    "    \n",
    "    else:\n",
    "        spam_smoothing = 0\n",
    "        ham_smoothing = 0\n",
    "    anw = Predict(test_word_vector, p_spam_vector, p_spam, p_ham_vector, p_ham, spam_smoothing, ham_smoothing)\n",
    "    prediction.append(anw)\n",
    "\n",
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f7c9978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97847533632287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       949\n",
      "        spam       0.98      0.87      0.92       166\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.94      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n",
      "[[946   3]\n",
      " [ 21 145]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(accuracy_score(labels_test, prediction))\n",
    "print(classification_report(labels_test, prediction))\n",
    "print(confusion_matrix(labels_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5726c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
